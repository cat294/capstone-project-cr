{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "Eating out Places in Chennai"}, {"metadata": {}, "cell_type": "markdown", "source": "Due to the Development of Chennai as an IT Hub, employees are being trasferred to Chennai in large numbers. \nThe problem lies in the fact that the employees who will be moving there may not be able to get good restaurants in the neighbourhoods. So this study is to find out if there are ample number of restaurants in the area so that the employees can live in those areas that have abundant restaurants. "}, {"metadata": {}, "cell_type": "markdown", "source": "Introduction\n\nThe aim of he study is to find neighborhoods with large number of restaurants/food stalls/cafes. Firstly, the number of neighborhoods and their respective coordinates need to be retrieved, so that Foursquare can find nearby venues. Using this data, Foursquare should search for nearby venues and get their categories.\n\nThese venues are then clustered using k-means. The cluster in which eateries are of the highest frequency will be the set of neighborhoods we are looking for. All of these neighborhoods are suitable for the employees to move in.\n\nThis problem can also be easily extended to fit other requests, such as finding the neighborhoods with low flat prices, neighborhoods with a wide variety of shops and malls, neighborhoods with good access to public transport systems etc.\n\nThe target audience here are employees who are moving to a new city and require some knowledge about the neighborhoods beforehand so that they can decide the place they want to live in.\n"}, {"metadata": {}, "cell_type": "code", "source": "import requests\nimport pandas as pd\nfrom pandas.io.json import json_normalize\nimport numpy as np\nimport random\nfrom tqdm import tqdm_notebook\nimport folium\nfrom geopy.geocoders import Nominatim\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#from IPython.display import Image \n#from IPython.core.display import HTML \n#from pandas.io.json import json_normalize", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Webscraping of data from Wikipedia"}, {"metadata": {}, "cell_type": "code", "source": "url = 'https://en.wikipedia.org/wiki/Areas_of_Chennai'\npage_unparsed = urllib.request.urlopen(url)\nsoup = BeautifulSoup(page_unparsed, 'html.parser')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "wiki_rows = [] # each row in the wikipedia table\nurls = []\nnames = []\n\nwiki_table = soup.find_all(\"table\", {\"class\": \"wikitable\"})\nfor row in wiki_table:\n  wiki_rows.append(row.find_all('a', href=True))\n\n# gets names and links of each neighborhood so that further scraping can be done\nfor i in range(len(wiki_rows[0])):\n  urls.append('https://en.wikipedia.org' + wiki_rows[0][i]['href'])\n  names.append(wiki_rows[0][i].text)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# getting data from each neighborhood\n\nlatitudes = []\nlongitudes = []\npincodes = []\n\nfor url in tqdm_notebook(urls, total = len(urls), unit = 'url'):\n  try: # because some links are broken\n    page_unparsed = urllib.request.urlopen(url)\n    soup = BeautifulSoup(page_unparsed, 'html.parser')\n  except:\n    continue\n\n  coords = soup.find(\"span\", {\"class\" : \"geo-dec\"})\n  pincode = soup.find(\"div\", {\"class\" : \"postal-code\"})\nif coords == None:  # because some pages do not have coordinates listed\n    latitudes.append(np.nan)\n    longitudes.append(np.nan)\n\n  else:\n    coords = coords.text.split()\n    latitudes.append(float(coords[0].replace('N', '').replace('\u00b0', '')))\n    longitudes.append(float(coords[1].replace('E', '').replace('\u00b0', '')))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "neighborhoods = pd.DataFrame(list(zip(names, latitudes, longitudes)), columns =['Name', 'Latitude', 'Longitude']) \nneighborhoods = neighborhoods[neighborhoods['Latitude'].notnull()]\nneighborhoods = neighborhoods[neighborhoods['Longitude'].notnull()]\nneighborhoods.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Foursquare\nUsing Foursquare, individual neighboords are searched to find nearby venues and their categories withing a 500m radius of a randomnly chosen neighborhood, Adyar.\n"}, {"metadata": {}, "cell_type": "code", "source": "CLIENT_ID = 'LDSODETW2HHHFM3RBS3VEN4ZHF1ZU05FCD11PDTBBAT1YR3U' # your Foursquare ID\nCLIENT_SECRET = 'KTYUBVGXKJY1FPY2YAWEXCWVM1R5EXR5TZMDOSYIMVYXSFN' # your Foursquare Secret\nVERSION = '20180605'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "neighborhood_latitude = neighborhoods[neighborhoods['Name'] == 'Adyar']['Latitude']\nneighborhood_longitude = neighborhoods[neighborhoods['Name'] == 'Adyar']['Longitude']\n\nurl = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n    CLIENT_ID[::-1], \n    CLIENT_SECRET[::-1], \n    VERSION, \n    neighborhood_latitude, \n    neighborhood_longitude, \n    radius, \n    LIMIT)\n\nresults = requests.get(url).json()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def get_category_type(row):\n    try:\n        categories_list = row['categories']\n    except:\n        categories_list = row['venue.categories']\n        \n    if len(categories_list) == 0:\n        return None\n    else:\n        return categories_list[0]['name']", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "LIMIT = 100 # limit of number of venues returned by Foursquare API\nradius = 500 # \n\ndef getNearbyVenues(names, latitudes, longitudes, radius=500):\n    venues_list = []\n    for name, lat, lng in tqdm_notebook(zip(names, latitudes, longitudes), total = neighborhoods.shape[0], unit = 'neighborhoods'):\n        # print(name)\nurl = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n            CLIENT_ID[::-1], \n            CLIENT_SECRET[::-1], \n            VERSION, \n            lat, \n            lng, \n            radius, \n            LIMIT)\n            \n        # make the GET request\n        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n                url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n            CLIENT_ID[::-1], \n            CLIENT_SECRET[::-1], \n            VERSION, \n            lat, \n            lng, \n            radius, \n            LIMIT)\n            \n        # make the GET request\n        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n        \n# return only relevant information for each nearby venue\n        venues_list.append([(\n            name, \n            lat, \n            lng, \n            v['venue']['name'], \n            v['venue']['categories'][0]['id'],\n            v['venue']['location']['lat'], \n            v['venue']['location']['lng'],  \n            v['venue']['categories'][0]['name']) for v in results])\n    return(nearby_venues)\n    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n    nearby_venues.columns = ['Neighborhood', \n                  'Neighborhood Latitude', \n                  'Neighborhood Longitude', \n                  'Venue', \n                  'Venue ID',\n                  'Venue Latitude', \n                  'Venue Longitude', \n                  'Venue Category']\n    return(nearby_venues)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "chennai_venues = getNearbyVenues(names = neighborhoods['Name'],\n                                   latitudes = neighborhoods['Latitude'],\n                                   longitudes = neighborhoods['Longitude'])\nchennai_venues.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Methodology - One hot encoding."}, {"metadata": {}, "cell_type": "code", "source": "chennai_onehot = pd.get_dummies(chennai_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\nchennai_onehot['Neighborhood'] = chennai_venues['Neighborhood'] \nfixed_columns = [chennai_onehot.columns[-1]] + list(chennai_onehot.columns[:-1])\nchennai_onehot = chennai_onehot[fixed_columns]\nchennai_onehot.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "chennai_grouped = chennai_onehot.groupby('Neighborhood').mean().reset_index()\nchennai_grouped", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Get the most frequent venues in each neighborhood."}, {"metadata": {}, "cell_type": "code", "source": "def return_most_common_venues(row, num_top_venues):\n    row_categories = row.iloc[1:]\n    row_categories_sorted = row_categories.sort_values(ascending=False)\n    \n    return row_categories_sorted.index.values[0:num_top_venues]\n\nnum_top_venues = 10\n\nindicators = ['st', 'nd', 'rd']\n\n# create columns according to number of top venues\ncolumns = ['Neighborhood']\nfor ind in np.arange(num_top_venues):\n    try:\n        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n    except:\n        columns.append('{}th Most Common Venue'.format(ind+1))\n\n# create a new dataframe\nneighborhoods_venues_sorted = pd.DataFrame(columns=columns)\nneighborhoods_venues_sorted['Neighborhood'] = chennai_grouped['Neighborhood']\n\nfor ind in np.arange(chennai_grouped.shape[0]):\n    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(chennai_grouped.iloc[ind, :], num_top_venues)\n\nneighborhoods_venues_sorted.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# set number of clusters\nkclusters = 4\n\nchennai_grouped_clustering = chennai_grouped.drop('Neighborhood', 1)\n\n# run k-means clustering\nkmeans = KMeans(n_clusters=kclusters, random_state=0).fit(chennai_grouped_clustering)\n\n# add clustering labels\nneighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)\nchennai_merged = neighborhoods\n\n# merge toronto_grouped with toronto_data to add latitude/longitude for each neighborhood\nchennai_merged = chennai_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Name')\n\nchennai_merged.head() # check the last columns!", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "chennai_merged = chennai_merged[chennai_merged['Cluster Labels'].notnull()]\n\n# create map\nmap_clusters = folium.Map(location=[13.067439, 80.237617], zoom_start=11)\n\n# set color scheme for the clusters\n'''\nx = np.arange(kclusters)\nys = [i + x + (i*x)**2 for i in range(kclusters+1)]\ncolors_array = cm.hsv(np.linspace(0, 1, len(ys)))\nhsv = [colors.rgb2hex(i) for i in colors_array]\n'''\n\ncolors = [\"#ff0000\", \"#3d84ad\", \"#000000\", \"#ffff00\"]\n\n# add markers to the map\nmarkers_colors = []\nfor lat, lon, poi, cluster in zip(chennai_merged['Latitude'], chennai_merged['Longitude'], chennai_merged['Name'], chennai_merged['Cluster Labels']):\n    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n    folium.CircleMarker(\n        [lat, lon],\n        radius=5,\n        popup=label,\n        color=colors[int(cluster)],\n        fill=True,\n        fill_color=colors[int(cluster)],\n        fill_opacity=0.7).add_to(map_clusters)\n       \n\nmap_clusters", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "chennai_merged.loc[chennai_merged['Cluster Labels'] == 0, chennai_merged.columns[[0] + list(range(4, chennai_merged.shape[1]))]]['1st Most Common Venue'].value_counts().head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "chennai_merged.loc[chennai_merged['Cluster Labels'] == 1, chennai_merged.columns[[0] + list(range(4, chennai_merged.shape[1]))]]['1st Most Common Venue'].value_counts().head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "chennai_merged.loc[chennai_merged['Cluster Labels'] == 2, chennai_merged.columns[[0] + list(range(4, chennai_merged.shape[1]))]]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "chennai_merged.loc[chennai_merged['Cluster Labels'] == 3, chennai_merged.columns[[0] + list(range(4, chennai_merged.shape[1]))]]['1st Most Common Venue'].value_counts().head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "\nResults\n\nThe following bar graph shows that Cluster 1 has the most number of restaurants."}, {"metadata": {}, "cell_type": "code", "source": "clus1 = pd.DataFrame(pd.DataFrame(list(chennai_merged[chennai_merged['Cluster Labels'] == 0].iloc[:, 4:15].values.ravel()), columns = ['venue_count'])['venue_count'].value_counts()[:6])\n\nflatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\nfig, ax = plt.subplots(figsize=(12, 9))\nax = sns.barplot(x = clus1.index, y = clus1['venue_count'], palette=(flatui))\nax.set_xticklabels(ax.get_xticklabels(), rotation = 30,  fontsize = 15)\nax.yaxis.label.set_size(15)\nplt.title('Most frequent venues in cluster 0', fontsize = 15)\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "The bar graphs for the rest of the clusters show that they do not have any significant similarities between each other.", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "clus1 = pd.DataFrame(pd.DataFrame(list(chennai_merged[chennai_merged['Cluster Labels'] == 1].iloc[:, 4:15].values.ravel()), columns = ['venue_count'])['venue_count'].value_counts()[:6])\nclus2 = pd.DataFrame(pd.DataFrame(list(chennai_merged[chennai_merged['Cluster Labels'] == 2].iloc[:, 4:15].values.ravel()), columns = ['venue_count'])['venue_count'].value_counts()[:6])\nclus3 = pd.DataFrame(pd.DataFrame(list(chennai_merged[chennai_merged['Cluster Labels'] == 3].iloc[:, 4:15].values.ravel()), columns = ['venue_count'])['venue_count'].value_counts()[:6])\n\nflatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\nfig, ax = plt.subplots(2,2, figsize = (20, 18))\n\nplt.subplot(2, 2, 1)\nax = sns.barplot(x = clus1.index, y = clus0['venue_count'], palette=(flatui))\nax.set_xticklabels(ax.get_xticklabels(), rotation = 30)\nplt.title('Most frequent venues in cluster 1')\n\nplt.subplot(2, 2, 2)\nax = sns.barplot(x = clus2.index, y = clus2['venue_count'], palette=(flatui))\nax.set_xticklabels(ax.get_xticklabels(), rotation = 30)\nplt.title('Most frequent venues in cluster 2')\n\nplt.subplot(2, 2, 3)\nax = sns.barplot(x = clus3.index, y = clus3['venue_count'], palette=(flatui))\nax.set_xticklabels(ax.get_xticklabels(), rotation = 30)\nplt.title('Most frequent venues in cluster 3', )\n\n\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Conclusion and Discussion "}, {"metadata": {}, "cell_type": "code", "source": "chennai_venues['Venue Category'].value_counts()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "There are quite a number of restaurants in Chennai so any new employee will not have any issue in finding good places to eat in the neighbourhood. "}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.10", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}